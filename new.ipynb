{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1czVdIlqnImH"
      },
      "source": [
        "# Super-resolution GAN (SRGAN)\n",
        "\n",
        "*Please note that this is an optional notebook meant to introduce more advanced concepts. If you’re up for a challenge, take a look and don’t worry if you can’t follow everything. There is no code to implement—only some cool code for you to learn and run!*\n",
        "\n",
        "It is recommended that you should already be familiar with:\n",
        " - Residual blocks, from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) (He et al. 2015)\n",
        " - Perceptual loss, from [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155) (Johnson et al. 2016)\n",
        " - VGG architecture, from [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556) (Simonyan et al. 2015)\n",
        "\n",
        "### Goals\n",
        "\n",
        "In this notebook, you will learn about Super-Resolution GAN (SRGAN), a GAN that enhances the resolution of images by 4x, proposed in [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802) (Ledig et al. 2017). You will also implement the architecture and training in full and be able to train it on the CIFAR dataset.\n",
        "\n",
        "### Background\n",
        "\n",
        "The authors first train a super-resolution residual network (SRResNet) with standard pixel-wise loss that achieves state-of-the-art metrics. They then insert this as the generator in the SRGAN framework, which is trained with a combination of pixel-wise, perceptual, and adversarial losses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SeMjdGIICPB"
      },
      "source": [
        "## SRGAN Submodules\n",
        "\n",
        "Before jumping into SRGAN, let's first take a look at some components that will be useful later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# Set random seeds\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9zFUlwLIOcC"
      },
      "source": [
        "### Parametric ReLU (PReLU)\n",
        "\n",
        "As you already know, ReLU is one of the simplest activation functions that can be described as\n",
        "\n",
        "\\begin{align*}\n",
        "    x_{\\text{ReLU}} := \\max(0, x),\n",
        "\\end{align*}\n",
        "\n",
        "where negative values of $x$ become thresholded at $0$. However, this stops gradient through these negative values, which can hinder training. The authors of [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852) addressed this by introducing a more general ReLU by scaling negative values by some constant $a > 0$:\n",
        "\n",
        "\\begin{align*}\n",
        "    x_{\\text{PReLU}} := \\max(0, x) + a * \\min(0, x).\n",
        "\\end{align*}\n",
        "\n",
        "Conveniently, this is implemented in Pytorch as [torch.nn.PReLU](https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB1Vq8ps7Bfd"
      },
      "source": [
        "### Residual Blocks\n",
        "\n",
        "The residual block, which is relevant in many state-of-the-art computer vision models, is used in all parts of SRGAN and is similar to the ones used in Pix2PixHD (see optional notebook). If you're not familiar with residual blocks, please take a look [here](https://paperswithcode.com/method/residual-block). Now, you'll start by first implementing a basic residual block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GHD_wif07f4b"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    '''\n",
        "    ResidualBlock Class\n",
        "    Values\n",
        "        channels: the number of channels throughout the residual block, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.PReLU(),\n",
        "\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCBCYvalE04U"
      },
      "source": [
        "###  PixelShuffle\n",
        "\n",
        "Proposed in [Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network](https://arxiv.org/abs/1609.05158) (Shi et al. 2016), PixelShuffle, also called sub-pixel convolution, is another way to upsample an image.\n",
        "\n",
        "PixelShuffle simply reshapes a $r^2C\\ x\\ H\\ x\\ W$ tensor into a $C\\ x\\ rH\\ x\\ rW$ tensor, essentially trading channel information for spatial information. Instead of convolving with stride $1/r$ as in deconvolution, the authors think about the weights in the kernel as being spaced $1/r$ pixels apart. When sliding this kernel over an input, the weights that fall between pixels aren't activated and don't need need to be calculated. The total number of activation patterns is thus increased by a factor of $r^2$. This operation is illustrated in the figure below.\n",
        "\n",
        "Don't worry if this is confusing! The algorithm is conveniently implemented as `torch.nn.PixelShuffle` in PyTorch, so as long as you have a general idea of how this works, you're set.\n",
        "\n",
        "> ![Efficient Sub-pixel CNN](https://github.com/https-deeplearning-ai/GANs-Public/blob/master/SRGAN-PixelShuffle.png?raw=true)\n",
        "*Efficient sub-pixel CNN, taken from Figure 1 of [Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network](https://arxiv.org/abs/1609.05158) (Shi et al. 2016). The PixelShuffle operation (also known as sub-pixel convolution) is shown as the last step on the right.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDSSM7SJJgon"
      },
      "source": [
        "## SRGAN Parts\n",
        "\n",
        "Now that you've learned about the various SRGAN submodules, you can now use them to build the generator and discriminator!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ41VxyyIaes"
      },
      "source": [
        "### Generator (SRResNet)\n",
        "\n",
        "The super-resolution residual network (SRResNet) and the generator are the same thing. The generator network architecture is actually quite simple - just a bunch of convolutional layers, residual blocks, and pixel shuffling layers!\n",
        "\n",
        "> ![SRGAN Generator](https://github.com/https-deeplearning-ai/GANs-Public/blob/master/SRGAN-Generator.png?raw=true)\n",
        "*SRGAN Generator, taken from Figure 4 of [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802) (Ledig et al. 2017).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EXqRw0f7KO81"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    '''\n",
        "    Generator Class\n",
        "    Values:\n",
        "        base_channels: number of channels throughout the generator, a scalar\n",
        "        n_ps_blocks: number of PixelShuffle blocks, a scalar\n",
        "        n_res_blocks: number of residual blocks, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, base_channels=64, n_ps_blocks=2, n_res_blocks=16):\n",
        "        super().__init__()\n",
        "        # Input layer\n",
        "        self.in_layer = nn.Sequential(\n",
        "            nn.Conv2d(3, base_channels, kernel_size=9, padding=4),\n",
        "            nn.PReLU(),\n",
        "        )\n",
        "\n",
        "        # Residual blocks\n",
        "        res_blocks = []\n",
        "        for _ in range(n_res_blocks):\n",
        "            res_blocks += [ResidualBlock(base_channels)]\n",
        "\n",
        "        res_blocks += [\n",
        "            nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(base_channels),\n",
        "        ]\n",
        "        self.res_blocks = nn.Sequential(*res_blocks)\n",
        "\n",
        "        # PixelShuffle blocks\n",
        "        ps_blocks = []\n",
        "        for _ in range(n_ps_blocks):\n",
        "            ps_blocks += [\n",
        "                nn.Conv2d(base_channels, 4 * base_channels, kernel_size=3, padding=1),\n",
        "                nn.PixelShuffle(2),\n",
        "                nn.PReLU(),\n",
        "            ]\n",
        "        self.ps_blocks = nn.Sequential(*ps_blocks)\n",
        "\n",
        "        # Output layer\n",
        "        self.out_layer = nn.Sequential(\n",
        "            nn.Conv2d(base_channels, 3, kernel_size=9, padding=4),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_res = self.in_layer(x)\n",
        "        x = x_res + self.res_blocks(x_res)\n",
        "        x = self.ps_blocks(x)\n",
        "        x = self.out_layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbpiSiWpNjtR"
      },
      "source": [
        "### Discriminator\n",
        "\n",
        "The discriminator architecture is also relatively straightforward, just one big sequential model - see the diagram below for reference!\n",
        "\n",
        "![SRGAN Generator](https://github.com/https-deeplearning-ai/GANs-Public/blob/master/SRGAN-Discriminator.png?raw=true)\n",
        "*SRGAN Discriminator, taken from Figure 4 of [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802) (Ledig et al. 2017).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cpyEsLi-OQj4"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    '''\n",
        "    Discriminator Class\n",
        "    Values:\n",
        "        base_channels: number of channels in first convolutional layer, a scalar\n",
        "        n_blocks: number of convolutional blocks, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, base_channels=64, n_blocks=3):\n",
        "        super().__init__()\n",
        "        self.blocks = [\n",
        "            nn.Conv2d(3, base_channels, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=1, stride=2),\n",
        "            nn.BatchNorm2d(base_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        ]\n",
        "\n",
        "        cur_channels = base_channels\n",
        "        for i in range(n_blocks):\n",
        "            self.blocks += [\n",
        "                nn.Conv2d(cur_channels, 2 * cur_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(2 * cur_channels),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "                nn.Conv2d(2 * cur_channels, 2 * cur_channels, kernel_size=3, padding=1, stride=2),\n",
        "                nn.BatchNorm2d(2 * cur_channels),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            ]\n",
        "            cur_channels *= 2\n",
        "\n",
        "        self.blocks += [\n",
        "            # You can replicate nn.Linear with pointwise nn.Conv2d\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(cur_channels, 2 * cur_channels, kernel_size=1, padding=0),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(2 * cur_channels, 1, kernel_size=1, padding=0),\n",
        "\n",
        "            # Apply sigmoid if necessary in loss function for stability\n",
        "            nn.Flatten(),\n",
        "        ]\n",
        "\n",
        "        self.layers = nn.Sequential(*self.blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzkNr4vgTGvb"
      },
      "source": [
        "## Loss Functions\n",
        "\n",
        "The authors formulate the perceptual loss as a weighted sum of content loss (based on the VGG19 network) and adversarial loss.\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathcal{L} &= \\mathcal{L}_{VGG} + 10^{-3}\\mathcal{L}_{ADV}\n",
        "\\end{align*}\n",
        "\n",
        "**Content Loss**\n",
        "\n",
        "Previous approaches have used MSE loss for content loss, but this objective function tends to produce blurry images. To address this, they add an extra MSE loss term on VGG19 feature maps. So for feature map $\\phi_{5,4}$ (the feature map after the 4th convolution before the 5th max-pooling layer) from the VGG19 network,\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathcal{L}_{VGG} &= \\left|\\left|\\phi_{5,4}(I^{\\text{HR}}) - \\phi_{5,4}(G(I^{\\text{LR}}))\\right|\\right|_2^2\n",
        "\\end{align*}\n",
        "\n",
        "where $I^{\\text{HR}}$ is the original high-resolution image and $I^{\\text{LR}}$ is the corresponding low-resolution image.\n",
        "\n",
        "**Adversarial Loss**\n",
        "\n",
        "You should already be familiar with adversarial loss, which is formulated as\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathcal{L}_{ADV} &= \\sum_{n=1}^N -\\log D(G(I^{\\text{LR}}))\n",
        "\\end{align*}\n",
        "\n",
        "Note that $-\\log D(G(\\cdot))$ is used instead of $\\log [1 - D(G(\\cdot))]$ for better gradient behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Mv2KIqHgJ3gh"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import vgg19\n",
        "\n",
        "class Loss(nn.Module):\n",
        "    '''\n",
        "    Loss Class\n",
        "    Implements composite content+adversarial loss for SRGAN\n",
        "    Values:\n",
        "        device: 'cuda' or 'cpu' hardware to put VGG network on, a string\n",
        "    '''\n",
        "\n",
        "    def __init__(self, device='cuda'):\n",
        "        super().__init__()\n",
        "\n",
        "        vgg = vgg19(pretrained=True).to(device)\n",
        "        self.vgg = nn.Sequential(*list(vgg.features)[:-1]).eval()\n",
        "        for p in self.vgg.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    @staticmethod\n",
        "    def img_loss(x_real, x_fake):\n",
        "        return F.mse_loss(x_real, x_fake)\n",
        "\n",
        "    def adv_loss(self, x, is_real):\n",
        "        target = torch.zeros_like(x) if is_real else torch.ones_like(x)\n",
        "        return F.binary_cross_entropy_with_logits(x, target)\n",
        "\n",
        "    def vgg_loss(self, x_real, x_fake):\n",
        "        return F.mse_loss(self.vgg(x_real), self.vgg(x_fake))\n",
        "\n",
        "    def forward(self, generator, discriminator, hr_real, lr_real):\n",
        "        ''' Performs forward pass and returns total losses for G and D '''\n",
        "        hr_fake = generator(lr_real)\n",
        "        fake_preds_for_g = discriminator(hr_fake)\n",
        "        fake_preds_for_d = discriminator(hr_fake.detach())\n",
        "        real_preds_for_d = discriminator(hr_real.detach())\n",
        "\n",
        "        g_loss = (\n",
        "            0.001 * self.adv_loss(fake_preds_for_g, False) + \\\n",
        "            0.006 * self.vgg_loss(hr_real, hr_fake) + \\\n",
        "            self.img_loss(hr_real, hr_fake)\n",
        "        )\n",
        "        d_loss = 0.5 * (\n",
        "            self.adv_loss(real_preds_for_d, True) + \\\n",
        "            self.adv_loss(fake_preds_for_d, False)\n",
        "        )\n",
        "\n",
        "        return g_loss, d_loss, hr_fake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAYCRF1cS8DM"
      },
      "source": [
        "## Training SRGAN\n",
        "\n",
        "Now it's time to train your SRGAN! Let's first begin by defining our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DaiSLB8UTFWV"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# We are using STL (for speed and also since ImageNet is no longer publicly available)\n",
        "USING_STL = True\n",
        "if USING_STL:\n",
        "    DatasetSubclass = torchvision.datasets.STL10\n",
        "else:\n",
        "    DatasetSubclass = torchvision.datasets.ImageNet\n",
        "\n",
        "class Dataset(DatasetSubclass):\n",
        "    '''\n",
        "    Dataset Class\n",
        "    Implements a general dataset class for STL10 and ImageNet\n",
        "    Values:\n",
        "        hr_size: spatial size of high-resolution image, a list/tuple\n",
        "        lr_size: spatial size of low-resolution image, a list/tuple\n",
        "        *args/**kwargs: all other arguments for subclassed torchvision dataset\n",
        "    '''\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        hr_size = kwargs.pop('hr_size', [96, 96])\n",
        "        lr_size = kwargs.pop('lr_size', [24, 24])\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        if hr_size is not None and lr_size is not None:\n",
        "            assert hr_size[0] == 4 * lr_size[0]\n",
        "            assert hr_size[1] == 4 * lr_size[1]\n",
        "\n",
        "        # High-res images are cropped and scaled to [-1, 1]\n",
        "        self.hr_transforms = transforms.Compose([\n",
        "            transforms.RandomCrop(hr_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.Lambda(lambda img: np.array(img)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ])\n",
        "\n",
        "        # Low-res images are downsampled with bicubic kernel and scaled to [0, 1]\n",
        "        self.lr_transforms = transforms.Compose([\n",
        "            transforms.Normalize((-1.0, -1.0, -1.0), (2.0, 2.0, 2.0)),\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(lr_size, interpolation=Image.BICUBIC),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        self.to_pil = transforms.ToPILImage()\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Uncomment the following lines if you're using ImageNet\n",
        "        # path, label = self.imgs[idx]\n",
        "        # image = Image.open(path).convert('RGB')\n",
        "\n",
        "        # Uncomment the following if you're using STL\n",
        "        image = torch.from_numpy(self.data[idx])\n",
        "        image = self.to_pil(image)\n",
        "\n",
        "        hr = self.hr_transforms(image)\n",
        "        lr = self.lr_transforms(hr)\n",
        "        return hr, lr\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        hrs, lrs = [], []\n",
        "\n",
        "        for hr, lr in batch:\n",
        "            hrs.append(hr)\n",
        "            lrs.append(lr)\n",
        "\n",
        "        return torch.stack(hrs, dim=0), torch.stack(lrs, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall that the generator (SRResNet) is first trained alone with MSE loss and is combined with the discriminator and trained as SRGAN after. Check out the training loops below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Image size should be larger than 160 due to the 4 downsamplings in ms-ssim",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/jun/hcmut/SRGAN/C3W2_SRGAN_(Optional) copy.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m img1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m64\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m96\u001b[39m, \u001b[39m96\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m img2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m64\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m96\u001b[39m, \u001b[39m96\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(msssim(img1, img2))\n",
            "\u001b[1;32m/home/jun/hcmut/SRGAN/C3W2_SRGAN_(Optional) copy.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m img1 \u001b[39m=\u001b[39m resize(img1)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m img2 \u001b[39m=\u001b[39m resize(img2)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ms_ssim(img1, img2, data_range\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, size_average\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[0;32m~/anaconda3/envs/hcmut/lib/python3.8/site-packages/pytorch_msssim/ssim.py:200\u001b[0m, in \u001b[0;36mms_ssim\u001b[0;34m(X, Y, data_range, size_average, win_size, win_sigma, win, weights, K)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWindow size should be odd.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    199\u001b[0m smaller_side \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(X\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:])\n\u001b[0;32m--> 200\u001b[0m \u001b[39massert\u001b[39;00m smaller_side \u001b[39m>\u001b[39m (win_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m (\n\u001b[1;32m    201\u001b[0m     \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m    202\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mImage size should be larger than \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m due to the 4 downsamplings in ms-ssim\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m ((win_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m (\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m4\u001b[39m))\n\u001b[1;32m    204\u001b[0m \u001b[39mif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     weights \u001b[39m=\u001b[39m [\u001b[39m0.0448\u001b[39m, \u001b[39m0.2856\u001b[39m, \u001b[39m0.3001\u001b[39m, \u001b[39m0.2363\u001b[39m, \u001b[39m0.1333\u001b[39m]\n",
            "\u001b[0;31mAssertionError\u001b[0m: Image size should be larger than 160 due to the 4 downsamplings in ms-ssim"
          ]
        }
      ],
      "source": [
        "from pytorch_msssim import ms_ssim\n",
        "\n",
        "\n",
        "def msssim(img1, img2):\n",
        "    # X = (img1 + 1) / 2  # [-1, 1] => [0, 1]\n",
        "    # Y = (img2 + 1) / 2  \n",
        "    resize = transforms.Resize((160, 160))\n",
        "    img1 = resize(img1)\n",
        "    img2 = resize(img2)\n",
        "    return ms_ssim(img1, img2, data_range=1, size_average=True) #(N,)\n",
        "\n",
        "img1 = torch.rand(64, 3, 96, 96)\n",
        "img2 = torch.rand(64, 3, 96, 96)\n",
        "print(msssim(img1, img2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FTEi1FAgbCQG"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from pytorch_msssim import ms_ssim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "# Parse torch version for autocast\n",
        "# ######################################################\n",
        "version = torch.__version__\n",
        "version = tuple(int(n) for n in version.split('.')[:-1])\n",
        "has_autocast = version >= (1, 6)\n",
        "# ######################################################\n",
        "\n",
        "\n",
        "\n",
        "def show_tensor_images(image_tensor):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\n",
        "    size per image, plots and prints the images in an uniform grid.\n",
        "    '''\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat[:4], nrow=4)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()\n",
        "\n",
        "def psnr(img1, img2):\n",
        "\tmse = torch.mean(((img1 + 1) / 2 - (img2) / 2) ** 2).item()\n",
        "\tif mse < 1.0e-10:\n",
        "\t\treturn 100\n",
        "\tPIXEL_MAX = 1\n",
        "\treturn 20 * math.log10(PIXEL_MAX / math.sqrt(mse))\n",
        "\n",
        "def msssim(img1, img2):\n",
        "    X = (img1 + 1) / 2  # [-1, 1] => [0, 1]\n",
        "    Y = (img2 + 1) / 2  \n",
        "    return ms_ssim( X, Y, data_range=1, size_average=False ) #(N,)\n",
        "\n",
        "def train_srresnet(srresnet, dataloader, device, lr=1e-4, total_steps=1e6, display_step=500):\n",
        "    srresnet = srresnet.to(device).train()\n",
        "    optimizer = torch.optim.Adam(srresnet.parameters(), lr=lr)\n",
        "\n",
        "    cur_step = 0\n",
        "    mean_loss = 0.0\n",
        "    \n",
        "    # writer = SummaryWriter()\n",
        "\n",
        "    while cur_step < total_steps:\n",
        "        for hr_real, lr_real in tqdm(dataloader, position=0):\n",
        "            hr_real = hr_real.to(device)\n",
        "            lr_real = lr_real.to(device)\n",
        "\n",
        "            # Enable autocast to FP16 tensors (new feature since torch==1.6.0)\n",
        "            # If you're running older versions of torch, comment this out\n",
        "            # and use NVIDIA apex for mixed/half precision training\n",
        "            if has_autocast:\n",
        "                with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
        "                    hr_fake = srresnet(lr_real)\n",
        "                    loss = Loss.img_loss(hr_real, hr_fake)\n",
        "            else:\n",
        "                hr_fake = srresnet(lr_real)\n",
        "                loss = Loss.img_loss(hr_real, hr_fake)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            mean_loss += loss.item() / display_step\n",
        "\n",
        "            if cur_step % display_step == 0 and cur_step > 0:\n",
        "                print(hr_real.shape, hr_fake.shape)\n",
        "                psnr_score = psnr(hr_real, hr_fake)\n",
        "                msssim_score = ms_ssim(hr_real, hr_fake.to(hr_real.dtype))\n",
        "                print('Step {}: SRResNet loss: {:.5f}'.format(cur_step, mean_loss))\n",
        "                print('Step {}: PSNR: {:.5f}'.format(cur_step, psnr_score))\n",
        "                print('Step {}: MSSIM: {:.5f}'.format(cur_step, msssim_score))\n",
        "                # writer.add_scalar(f'Train/loss', mean_loss, cur_step)\n",
        "                show_tensor_images(lr_real * 2 - 1)\n",
        "                show_tensor_images(hr_fake.to(hr_real.dtype))\n",
        "                show_tensor_images(hr_real)\n",
        "                mean_loss = 0.0\n",
        "\n",
        "            cur_step += 1\n",
        "            if cur_step == total_steps:\n",
        "                break\n",
        "\n",
        "def train_srgan(generator, discriminator, dataloader, device, lr=1e-4, total_steps=2e5, display_step=500):\n",
        "    generator = generator.to(device).train()\n",
        "    discriminator = discriminator.to(device).train()\n",
        "    loss_fn = Loss(device=device)\n",
        "\n",
        "    g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
        "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
        "    g_scheduler = torch.optim.lr_scheduler.LambdaLR(g_optimizer, lambda _: 0.1)\n",
        "    d_scheduler = torch.optim.lr_scheduler.LambdaLR(d_optimizer, lambda _: 0.1)\n",
        "\n",
        "    lr_step = total_steps // 2\n",
        "    cur_step = 0\n",
        "\n",
        "    mean_g_loss = 0.0\n",
        "    mean_d_loss = 0.0\n",
        "\n",
        "    \n",
        "\n",
        "    while cur_step < total_steps:\n",
        "        for hr_real, lr_real in tqdm(dataloader, position=0):\n",
        "            hr_real = hr_real.to(device)\n",
        "            lr_real = lr_real.to(device)\n",
        "\n",
        "            # Enable autocast to FP16 tensors (new feature since torch==1.6.0)\n",
        "            # If you're running older versions of torch, comment this out\n",
        "            # and use NVIDIA apex for mixed/half precision training\n",
        "            if has_autocast:\n",
        "                with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
        "                    g_loss, d_loss, hr_fake = loss_fn(\n",
        "                        generator, discriminator, hr_real, lr_real,\n",
        "                    )\n",
        "            else:\n",
        "                g_loss, d_loss, hr_fake = loss_fn(\n",
        "                    generator, discriminator, hr_real, lr_real,\n",
        "                )\n",
        "\n",
        "            g_optimizer.zero_grad()\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            d_optimizer.zero_grad()\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            mean_g_loss += g_loss.item() / display_step\n",
        "            mean_d_loss += d_loss.item() / display_step\n",
        "\n",
        "            if cur_step == lr_step:\n",
        "                g_scheduler.step()\n",
        "                d_scheduler.step()\n",
        "                print('Decayed learning rate by 10x.')\n",
        "\n",
        "            if cur_step % display_step == 0 and cur_step > 0:\n",
        "                print('Step {}: Generator loss: {:.5f}, Discriminator loss: {:.5f}'.format(cur_step, mean_g_loss, mean_d_loss))\n",
        "                show_tensor_images(lr_real * 2 - 1)\n",
        "                show_tensor_images(hr_fake.to(hr_real.dtype))\n",
        "                show_tensor_images(hr_real)\n",
        "                mean_g_loss = 0.0\n",
        "                mean_d_loss = 0.0\n",
        "\n",
        "            cur_step += 1\n",
        "            if cur_step == total_steps:\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyypDt_UhBjz"
      },
      "source": [
        "Now initialize everything and run training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ScH0Iok8fAMS",
        "outputId": "db2fecd1-b066-4475-9060-9acb7df6e3f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 10/79 [00:01<00:10,  6.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 3, 96, 96]) torch.Size([64, 3, 96, 96])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Image size should be larger than 160 due to the 4 downsamplings in ms-ssim",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/jun/hcmut/SRGAN/C3W2_SRGAN_(Optional) copy.ipynb Cell 20\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     Dataset(\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, hr_size\u001b[39m=\u001b[39m[\u001b[39m96\u001b[39m, \u001b[39m96\u001b[39m], lr_size\u001b[39m=\u001b[39m[\u001b[39m24\u001b[39m, \u001b[39m24\u001b[39m]),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# train_srresnet(generator, dataloader, device, lr=1e-4, total_steps=1e5, display_step=1000)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m train_srresnet(generator, dataloader, device, lr\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m, total_steps\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, display_step\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m torch\u001b[39m.\u001b[39msave(generator, \u001b[39m'\u001b[39m\u001b[39msrresnet2.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;32m/home/jun/hcmut/SRGAN/C3W2_SRGAN_(Optional) copy.ipynb Cell 20\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39mprint\u001b[39m(hr_real\u001b[39m.\u001b[39mshape, hr_fake\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m psnr_score \u001b[39m=\u001b[39m psnr(hr_real, hr_fake)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m msssim_score \u001b[39m=\u001b[39m ms_ssim(hr_real, hr_fake\u001b[39m.\u001b[39;49mto(hr_real\u001b[39m.\u001b[39;49mdtype))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mStep \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: SRResNet loss: \u001b[39m\u001b[39m{:.5f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(cur_step, mean_loss))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mStep \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: PSNR: \u001b[39m\u001b[39m{:.5f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(cur_step, psnr_score))\n",
            "File \u001b[0;32m~/anaconda3/envs/hcmut/lib/python3.8/site-packages/pytorch_msssim/ssim.py:200\u001b[0m, in \u001b[0;36mms_ssim\u001b[0;34m(X, Y, data_range, size_average, win_size, win_sigma, win, weights, K)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWindow size should be odd.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    199\u001b[0m smaller_side \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(X\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:])\n\u001b[0;32m--> 200\u001b[0m \u001b[39massert\u001b[39;00m smaller_side \u001b[39m>\u001b[39m (win_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m (\n\u001b[1;32m    201\u001b[0m     \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m    202\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mImage size should be larger than \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m due to the 4 downsamplings in ms-ssim\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m ((win_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m (\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m4\u001b[39m))\n\u001b[1;32m    204\u001b[0m \u001b[39mif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     weights \u001b[39m=\u001b[39m [\u001b[39m0.0448\u001b[39m, \u001b[39m0.2856\u001b[39m, \u001b[39m0.3001\u001b[39m, \u001b[39m0.2363\u001b[39m, \u001b[39m0.1333\u001b[39m]\n",
            "\u001b[0;31mAssertionError\u001b[0m: Image size should be larger than 160 due to the 4 downsamplings in ms-ssim"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "generator = Generator(n_res_blocks=16, n_ps_blocks=2)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    Dataset('data', 'train', download=True, hr_size=[96, 96], lr_size=[24, 24]),\n",
        "    batch_size=64, pin_memory=True, shuffle=True,\n",
        ")\n",
        "# train_srresnet(generator, dataloader, device, lr=1e-4, total_steps=1e5, display_step=1000)\n",
        "train_srresnet(generator, dataloader, device, lr=1e-4, total_steps=100, display_step=10)\n",
        "torch.save(generator, 'srresnet2.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyypDt_UhBjz"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/jun/hcmut/SRGAN/C3W2_SRGAN_(Optional) copy.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m generator \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39msrresnet.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m discriminator \u001b[39m=\u001b[39m Discriminator(n_blocks\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, base_channels\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bjun/home/jun/hcmut/SRGAN/C3W2_SRGAN_%28Optional%29%20copy.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Uncomment the following lines if you're using STL\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/hcmut/lib/python3.8/site-packages/torch/serialization.py:601\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    599\u001b[0m orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n\u001b[1;32m    600\u001b[0m \u001b[39mwith\u001b[39;00m _open_zipfile_reader(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 601\u001b[0m     \u001b[39mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[1;32m    602\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch.load\u001b[39m\u001b[39m'\u001b[39m\u001b[39m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39m dispatching to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch.jit.load\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (call \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch.jit.load\u001b[39m\u001b[39m'\u001b[39m\u001b[39m directly to\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39m silence this warning)\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mUserWarning\u001b[39;00m)\n\u001b[1;32m    605\u001b[0m         opened_file\u001b[39m.\u001b[39mseek(orig_position)\n",
            "File \u001b[0;32m~/anaconda3/envs/hcmut/lib/python3.8/site-packages/torch/serialization.py:890\u001b[0m, in \u001b[0;36m_is_torchscript_zip\u001b[0;34m(zip_file)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_is_torchscript_zip\u001b[39m(zip_file):\n\u001b[0;32m--> 890\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mconstants.pkl\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m zip_file\u001b[39m.\u001b[39;49mget_all_records()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "generator = torch.load('srresnet2.pt')\n",
        "discriminator = Discriminator(n_blocks=1, base_channels=8)\n",
        "\n",
        "# Uncomment the following lines if you're using STL\n",
        "train_srgan(generator, discriminator, dataloader, device, lr=1e-4, total_steps=2e5, display_step=1000)\n",
        "torch.save(generator, 'srgenerator2.pt')\n",
        "torch.save(discriminator, 'srdiscriminator2.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
